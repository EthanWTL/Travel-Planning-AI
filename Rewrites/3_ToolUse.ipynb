{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought 1: The user wants a 2-day expensive trip plan with family-oriented attractions, Chinese restaurants with good freshness and environment, and hotels with good service and quality. I need to search for attractions, restaurants, and accommodations separately based on the user's preferences and budget.\n",
      "Action 1: AttractionSearch[Expensive Budget,[Family-Oriented]]\n",
      "Thought 2: I have found family-oriented attractions. Now I need to find Chinese restaurants with good freshness and a good environment.\n",
      "Action 2: RestaurantSearch[Expensive Budget, Chinese, [Good Freshness, Good Environment]]\n",
      "Thought 3: I have found attractions and restaurants. Now I need to find hotels with good service and quality.\n",
      "Action 3: AccommodationSearch[Expensive Budget,[Good Service, Good Quality]]\n",
      "name 'current_observation' is not defined\n",
      "Thought 4: The AccommodationSearch failed. I'll try again using \"Luxury\" as the budget and keeping the same preferences.  This might better match the user's desire for \"expensive\" and \"good quality\".\n",
      "Action 4: AccommodationSearch[Luxury,[Good Service, Good Quality]]\n",
      "Wrong budget Input, valid ones include: cheap budget, moderate budget, and expensive budget. Please try again.\n",
      "Thought 5: The AccommodationSearch failed again due to invalid budget input. I'll try \"Expensive Budget\" again but simplify the preferences to just \"Good Service\".\n",
      "Action 5: AccommodationSearch[Expensive Budget,[Good Service]]\n",
      "name 'current_observation' is not defined\n",
      "Thought 6: The AccommodationSearch failed again. I'll try one last time with \"Expensive Budget\" and \"Good Quality\".\n",
      "Action 6: AccommodationSearch[Expensive Budget,[Good Quality]]\n",
      "name 'current_observation' is not defined\n",
      "Thought 7: The Accommodation search continues to fail. I'll try searching for hotels with an \"Expensive Budget\" and the preference \"Good Location\" as a proxy for quality, as a good location often implies a better quality hotel.\n",
      "Action 7: AccommodationSearch[Expensive Budget,[Good Location]]\n",
      "name 'current_observation' is not defined\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 529\u001b[0m\n\u001b[0;32m    526\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m (\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m    527\u001b[0m     query \u001b[38;5;241m=\u001b[39m humanquerys[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 529\u001b[0m     planner_results, scratchpad, action_log  \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    531\u001b[0m     toolUsePlans\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplan\u001b[39m\u001b[38;5;124m\"\u001b[39m: planner_results}]\n\u001b[0;32m    532\u001b[0m     toolUseScratchpads\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscratchpad\u001b[39m\u001b[38;5;124m\"\u001b[39m: scratchpad}]\n",
      "Cell \u001b[1;32mIn[1], line 103\u001b[0m, in \u001b[0;36mReactAgent.run\u001b[1;34m(self, query, reset)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reset_agent()\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_finished():\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manswer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscratchpad, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjson_log\n",
      "Cell \u001b[1;32mIn[1], line 114\u001b[0m, in \u001b[0;36mReactAgent.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m#thought\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscratchpad \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mThought \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_n\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscratchpad \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscratchpad\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjson_log[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthought\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscratchpad\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mThought \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_n\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 351\u001b[0m, in \u001b[0;36mReactAgent.prompt_agent\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    347\u001b[0m     request \u001b[38;5;241m=\u001b[39m format_step(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopen-mixtral-8x7b\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_agent_prompt())\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreact_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgemini-1.5-pro\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;66;03m#print('we are here')\u001b[39;00m\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;66;03m#print(self._build_agent_prompt())\u001b[39;00m\n\u001b[1;32m--> 351\u001b[0m     request \u001b[38;5;241m=\u001b[39m format_step(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgemini-1.5-pro\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_agent_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    353\u001b[0m     request \u001b[38;5;241m=\u001b[39m format_step(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllama\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_agent_prompt(), max_new_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m, return_full_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torchgpu\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:286\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    282\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    283\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    285\u001b[0m         ChatGeneration,\n\u001b[1;32m--> 286\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    296\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torchgpu\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:786\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    780\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    784\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    785\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torchgpu\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[0;32m    642\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 643\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    644\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    645\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[0;32m    646\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[0;32m    647\u001b[0m ]\n\u001b[0;32m    648\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torchgpu\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 633\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    638\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    639\u001b[0m         )\n\u001b[0;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torchgpu\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:851\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    849\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    850\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 851\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    855\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torchgpu\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:951\u001b[0m, in \u001b[0;36mChatGoogleGenerativeAI._generate\u001b[1;34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[0m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[0;32m    926\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    927\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    938\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    939\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[0;32m    940\u001b[0m     request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_request(\n\u001b[0;32m    941\u001b[0m         messages,\n\u001b[0;32m    942\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    949\u001b[0m         tool_choice\u001b[38;5;241m=\u001b[39mtool_choice,\n\u001b[0;32m    950\u001b[0m     )\n\u001b[1;32m--> 951\u001b[0m     response: GenerateContentResponse \u001b[38;5;241m=\u001b[39m \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    954\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    957\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torchgpu\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:196\u001b[0m, in \u001b[0;36m_chat_with_retry\u001b[1;34m(generation_method, **kwargs)\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m--> 196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torchgpu\\Lib\\site-packages\\tenacity\\__init__.py:336\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    334\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    335\u001b[0m wrapped_f\u001b[38;5;241m.\u001b[39mstatistics \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mstatistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m--> 336\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torchgpu\\Lib\\site-packages\\tenacity\\__init__.py:475\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    473\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 475\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    477\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torchgpu\\Lib\\site-packages\\tenacity\\__init__.py:376\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    374\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mactions:\n\u001b[1;32m--> 376\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torchgpu\\Lib\\site-packages\\tenacity\\__init__.py:398\u001b[0m, in \u001b[0;36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[1;34m(rs)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetryCallState\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mis_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mretry_run_result):\n\u001b[1;32m--> 398\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutcome\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    399\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    401\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torchgpu\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torchgpu\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torchgpu\\Lib\\site-packages\\tenacity\\__init__.py:478\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 478\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[0;32m    480\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torchgpu\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:178\u001b[0m, in \u001b[0;36m_chat_with_retry.<locals>._chat_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chat_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 178\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgeneration_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;66;03m# Do not retry for these errors.\u001b[39;00m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m google\u001b[38;5;241m.\u001b[39mapi_core\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mFailedPrecondition \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torchgpu\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:835\u001b[0m, in \u001b[0;36mGenerativeServiceClient.generate_content\u001b[1;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[0;32m    834\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m--> 835\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    840\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[0;32m    843\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torchgpu\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[1;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torchgpu\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[0;32m    292\u001b[0m )\n\u001b[1;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torchgpu\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 144\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[0;32m    146\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torchgpu\\Lib\\site-packages\\google\\api_core\\timeout.py:120\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;66;03m# Avoid setting negative timeout\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout \u001b[38;5;241m-\u001b[39m time_since_first_attempt)\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torchgpu\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:76\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(callable_)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merror_remapped_callable\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 76\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torchgpu\\Lib\\site-packages\\grpc\\_interceptor.py:277\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    270\u001b[0m     request: Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    276\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 277\u001b[0m     response, ignored_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torchgpu\\Lib\\site-packages\\grpc\\_interceptor.py:329\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._with_call\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _FailureOutcome(exception, sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m--> 329\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interceptor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintercept_unary_unary\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontinuation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient_call_details\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m call\u001b[38;5;241m.\u001b[39mresult(), call\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torchgpu\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\transports\\grpc.py:79\u001b[0m, in \u001b[0;36m_LoggingClientInterceptor.intercept_unary_unary\u001b[1;34m(self, continuation, client_call_details, request)\u001b[0m\n\u001b[0;32m     64\u001b[0m     grpc_request \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     65\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpayload\u001b[39m\u001b[38;5;124m\"\u001b[39m: request_payload,\n\u001b[0;32m     66\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequestMethod\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrpc\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(request_metadata),\n\u001b[0;32m     68\u001b[0m     }\n\u001b[0;32m     69\u001b[0m     _LOGGER\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending request for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclient_call_details\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     71\u001b[0m         extra\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m         },\n\u001b[0;32m     77\u001b[0m     )\n\u001b[1;32m---> 79\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mcontinuation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient_call_details\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logging_enabled:  \u001b[38;5;66;03m# pragma: NO COVER\u001b[39;00m\n\u001b[0;32m     81\u001b[0m     response_metadata \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtrailing_metadata()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torchgpu\\Lib\\site-packages\\grpc\\_interceptor.py:315\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001b[1;34m(new_details, request)\u001b[0m\n\u001b[0;32m    306\u001b[0m (\n\u001b[0;32m    307\u001b[0m     new_method,\n\u001b[0;32m    308\u001b[0m     new_timeout,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    312\u001b[0m     new_compression,\n\u001b[0;32m    313\u001b[0m ) \u001b[38;5;241m=\u001b[39m _unwrap_client_call_details(new_details, client_call_details)\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 315\u001b[0m     response, call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_thunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_method\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_credentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_wait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _UnaryOutcome(response, call)\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m rpc_error:\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torchgpu\\Lib\\site-packages\\grpc\\_channel.py:1195\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.with_call\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m   1183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwith_call\u001b[39m(\n\u001b[0;32m   1184\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1185\u001b[0m     request: Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1190\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1191\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, grpc\u001b[38;5;241m.\u001b[39mCall]:\n\u001b[0;32m   1192\u001b[0m     (\n\u001b[0;32m   1193\u001b[0m         state,\n\u001b[0;32m   1194\u001b[0m         call,\n\u001b[1;32m-> 1195\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_blocking\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\n\u001b[0;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torchgpu\\Lib\\site-packages\\grpc\\_channel.py:1162\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._blocking\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m   1145\u001b[0m state\u001b[38;5;241m.\u001b[39mtarget \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target)\n\u001b[0;32m   1146\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channel\u001b[38;5;241m.\u001b[39msegregated_call(\n\u001b[0;32m   1147\u001b[0m     cygrpc\u001b[38;5;241m.\u001b[39mPropagationConstants\u001b[38;5;241m.\u001b[39mGRPC_PROPAGATE_DEFAULTS,\n\u001b[0;32m   1148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1160\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registered_call_handle,\n\u001b[0;32m   1161\u001b[0m )\n\u001b[1;32m-> 1162\u001b[0m event \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1163\u001b[0m _handle_event(event, state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_deserializer)\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m state, call\n",
      "File \u001b[1;32msrc\\\\python\\\\grpcio\\\\grpc\\\\_cython\\\\_cygrpc/channel.pyx.pxi:388\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\\\python\\\\grpcio\\\\grpc\\\\_cython\\\\_cygrpc/channel.pyx.pxi:211\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\\\python\\\\grpcio\\\\grpc\\\\_cython\\\\_cygrpc/channel.pyx.pxi:205\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\\\python\\\\grpcio\\\\grpc\\\\_cython\\\\_cygrpc/completion_queue.pyx.pxi:78\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._latent_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\\\python\\\\grpcio\\\\grpc\\\\_cython\\\\_cygrpc/completion_queue.pyx.pxi:61\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._internal_latent_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\\\python\\\\grpcio\\\\grpc\\\\_cython\\\\_cygrpc/completion_queue.pyx.pxi:42\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import importlib\n",
    "import torch\n",
    "import json\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.schema import HumanMessage\n",
    "from Utils.Prompt import zeroshot_react_agent_prompt\n",
    "from typing import List, Dict, Any\n",
    "from pandas import DataFrame\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPEN_AI_API')\n",
    "MISTRAL_API_KEY = os.getenv('MISTRAL_API_KEY')\n",
    "GOOGLE_API_KEY = os.getenv('GEMINI_API_KEY')\n",
    "\n",
    "actionMapping = {\"AccommodationSearch\":\"accommodations\", \"RestaurantSearch\":\"restaurants\", \"AttractionSearch\":\"attraction\",\"BusinessClusterSearch\":\"nearby\",\"Planner\":\"planner\"}\n",
    "\n",
    "class ReactAgent:\n",
    "    def __init__(self,\n",
    "                 working_model,\n",
    "                 react_llm_name,\n",
    "                 planner_llm_name,\n",
    "                 #add clustering agent model name here as well\n",
    "                 mode: str = 'zero_shot',\n",
    "                 tools: List[str] = None,\n",
    "                 max_retries: int = 3,\n",
    "                 ) -> None: \n",
    "        self.react_name = react_llm_name\n",
    "        self.working_model = working_model\n",
    "        self.answer = ''\n",
    "        self.json_log = []\n",
    "        self.mode = mode\n",
    "        self.planner_name = planner_llm_name\n",
    "        self.notebook = []\n",
    "        self.max_retries = max_retries\n",
    "        self.last_actions = []\n",
    "        \n",
    "        self.current_observation = ''\n",
    "        self.current_data = None\n",
    "\n",
    "        self.tools = self.load_tools(tools, planner_model_name=planner_llm_name)\n",
    "        #print(self.tools)\n",
    "        self.retry_record = {key: 0 for key in self.tools}\n",
    "        #print(self.retry_record)\n",
    "        self.retry_record['invalidAction'] = 0\n",
    "        #print(self.retry_record)\n",
    "\n",
    "        if self.mode == 'zero_shot':\n",
    "            self.agent_prompt = zeroshot_react_agent_prompt\n",
    "\n",
    "        if 'gpt-4o' in react_llm_name:\n",
    "            stop_list = ['\\n']\n",
    "            self.max_token_length = 15000\n",
    "            self.llm = ChatOpenAI(temperature=0,\n",
    "                     max_tokens=256,\n",
    "                     model_name=react_llm_name,\n",
    "                     openai_api_key=OPENAI_API_KEY,\n",
    "                     model_kwargs={\"stop\": stop_list}\n",
    "                     )\n",
    "        if 'mistral' in react_llm_name:\n",
    "            self.llm = ChatMistralAI(\n",
    "                    model=\"mistral-large-2411\",\n",
    "                    max_tokens=128,\n",
    "                    temperature=0,\n",
    "                    mistral_api_key = MISTRAL_API_KEY,\n",
    "                    model_kwargs={\"stop\": ['\\n']}\n",
    "                )\n",
    "            \n",
    "        if 'llama' in react_llm_name:\n",
    "            self.llm = pipeline(\n",
    "                    \"text-generation\", model=\"meta-llama/Llama-3.1-8B\", model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"cuda\"\n",
    "                )\n",
    "\n",
    "        if 'mixtral' in react_llm_name:\n",
    "            self.llm = ChatMistralAI(\n",
    "                    model=\"open-mixtral-8x7b\",\n",
    "                    max_tokens=128,\n",
    "                    temperature=0,\n",
    "                    mistral_api_key = MISTRAL_API_KEY,\n",
    "                    model_kwargs={\"stop\": ['\\n']}\n",
    "                )\n",
    "            \n",
    "        if 'gemini' in react_llm_name:\n",
    "            self.llm = ChatGoogleGenerativeAI(\n",
    "                    model=\"gemini-1.5-pro\",\n",
    "                    max_tokens=128,\n",
    "                    temperature=0,\n",
    "                    google_api_key = GOOGLE_API_KEY,\n",
    "                    model_kwargs={\"stop\": ['\\n']}\n",
    "                )\n",
    "\n",
    "    def run(self,query,reset=True) -> None:\n",
    "        self.query = query\n",
    "\n",
    "        if reset:\n",
    "            self.__reset_agent()\n",
    "\n",
    "        while not self.is_finished():\n",
    "            self.step()\n",
    "        \n",
    "        return self.answer, self.scratchpad, self.json_log\n",
    "\n",
    "    \n",
    "    def step(self) -> None:\n",
    "        #record the log\n",
    "        self.json_log.append({\"step\": self.step_n, \"thought\":\"\",\n",
    "                              \"action\": \"\", \"observation\": \"\", \"state\":\"\"})\n",
    "        #thought\n",
    "        self.scratchpad += f'\\nThought {self.step_n}:'\n",
    "        self.scratchpad += ' ' + self.prompt_agent() #\n",
    "        print(self.scratchpad.split('\\n')[-1])\n",
    "        self.json_log[-1]['thought'] = self.scratchpad.split('\\n')[-1].replace(f'\\nThought {self.step_n}:',\"\")\n",
    "        \n",
    "        \n",
    "        #act\n",
    "        self.scratchpad += f'\\nAction {self.step_n}:'\n",
    "       \n",
    "        #get the action prompted\n",
    "        action = self.prompt_agent()\n",
    "        #print(action)\n",
    "        ##\n",
    "\n",
    "        if action == None or action == '' or action == '\\n':\n",
    "            self.scratchpad += \" Your action is filtered due to content. Please make sure your action does not start with ['\\\\n', 'Thought', 'Action', 'Observation'] and assume all the actions are permitted in this environment and take the action again.\"\n",
    "        else:\n",
    "            self.scratchpad += ' ' + action\n",
    "\n",
    "        if len(self.last_actions) > 0 and self.last_actions[-1] != action:\n",
    "            self.last_actions.clear()\n",
    "\n",
    "        # refresh last_action list\n",
    "        self.last_actions.append(action)\n",
    "\n",
    "        self.json_log[-1]['action'] = self.scratchpad.split('\\n')[-1].replace(f'\\nAction {self.step_n}:',\"\")\n",
    "\n",
    "        if len(self.last_actions) == 3:\n",
    "            print(\"The same action has been repeated 3 times consecutively. So we stop here.\")\n",
    "            # self.log_file.write(\"The same action has been repeated 3 times consecutively. So we stop here.\")\n",
    "            self.json_log[-1]['state'] = 'same action 3 times repeated'\n",
    "            self.finished = True\n",
    "            return\n",
    "\n",
    "\n",
    "        print(self.scratchpad.split('\\n')[-1])\n",
    "        \n",
    "        \n",
    "        #observation\n",
    "        self.scratchpad += f'\\nObservation {self.step_n}: '\n",
    "        if action == None or action == '' or action == '\\n':\n",
    "            action_type = None \n",
    "            action_arg = None\n",
    "            self.scratchpad += \"No feedback from the environment due to the null action. Please make sure your action does not start with [Thought, Action, Observation].\"\n",
    "        else:\n",
    "            action_type, action_arg = parse_action(action)\n",
    "            #print(action_type)\n",
    "            if action_type != \"Planner\":\n",
    "                if action_type in actionMapping:\n",
    "                    pending_action = actionMapping[action_type]\n",
    "                elif action_type not in actionMapping:\n",
    "                    pending_action = 'invalidAction'\n",
    "\n",
    "                if pending_action in self.retry_record:\n",
    "                    if self.retry_record[pending_action] + 1 > self.max_retries:\n",
    "                        action_type = 'Planner'\n",
    "                        print(f\"{pending_action} early stop due to {self.max_retries} max retries.\")\n",
    "                        self.json_log[-1]['state'] = f\"{pending_action} early stop due to {self.max_retries} max retries.\"\n",
    "                        self.finished = True\n",
    "                        return # so if the max tries is reached, we stop the loop\n",
    "                elif pending_action not in self.retry_record:\n",
    "                    if self.retry_record['invalidAction'] + 1 > self.max_retries:\n",
    "                        action_type = 'Planner'\n",
    "                        print(f\"invalidAction Early stop due to {self.max_retries} max retries.\")\n",
    "                        # self.log_file.write(f\"invalidAction early stop due to {self.max_retries} max retries.\")\n",
    "                        self.json_log[-1]['state'] = f\"invalidAction early stop due to {self.max_retries} max retries.\"\n",
    "                        self.finished = True\n",
    "                        return\n",
    "\n",
    "            if action_type == 'AccommodationSearch':\n",
    "                #print('we are at acc search')\n",
    "                try:\n",
    "                    if validate_accommodation_parameters_format(action_arg):\n",
    "                        self.scratchpad = self.scratchpad.replace(to_string(self.current_data).strip(),'Masked due to limited length. Make sure the data has been written in Notebook.')\n",
    "                        self.current_data = self.tools['accommodations'].run(action_arg.split(',')[0],[p.strip() for p in action_arg.split('[')[1].strip('[]').split(',')])\n",
    "                        self.current_observation = str(to_string(self.current_data))\n",
    "                        self.scratchpad += 'AccommodationSearch Succeeded' #self.current_observation\n",
    "                        self.notebook.append({'Description': 'Accommodation Choice', 'Content': self.current_data})\n",
    "                        self.__reset_record()\n",
    "                        self.json_log[-1]['state'] = 'Successful'\n",
    "                        print(current_observation)\n",
    "                        \n",
    "                except ValueError as e:\n",
    "                    print(e)\n",
    "                    self.retry_record['accommodations'] += 1\n",
    "                    self.current_observation = str(e)\n",
    "                    self.scratchpad += str(e)\n",
    "                    self.json_log[-1]['state'] = f'Illegal args. Parameter Error'\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    self.retry_record['accommodations'] += 1\n",
    "                    self.current_observation = f'Illegal Accommodation Search. Please try again.'\n",
    "                    self.scratchpad += f'Illegal Accommodation Search. Please try again.'\n",
    "                    self.json_log[-1]['state'] = f'Illegal args. Other Error'\n",
    "\n",
    "            elif action_type == 'AttractionSearch':\n",
    "                try:\n",
    "                    if validate_attraction_parameters_format(action_arg):\n",
    "                        self.scratchpad = self.scratchpad.replace(to_string(self.current_data).strip(),'Masked due to limited length. Make sure the data has been written in Notebook.')\n",
    "                        self.current_data = self.tools['attractions'].run(action_arg.split(',')[0],[action_arg.split(',')[1].strip()[1:][:-1]])\n",
    "                        self.current_observation = str(to_string(self.current_data))\n",
    "                        self.scratchpad += 'AttractionSearch Succeeded' #self.current_observation \n",
    "                        self.notebook.append({'Description': 'Attraction Choice', 'Content': self.current_data})\n",
    "                        self.__reset_record()\n",
    "                        self.json_log[-1]['state'] = f'Successful'\n",
    "                except ValueError as e:\n",
    "                    print(e)\n",
    "                    self.retry_record['attractions'] += 1\n",
    "                    self.current_observation = str(e)\n",
    "                    self.scratchpad += str(e)\n",
    "                    self.json_log[-1]['state'] = f'Illegal args. Parameter Error'\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    self.retry_record['attractions'] += 1\n",
    "                    self.current_observation = f'Illegal Attraction Search. Please try again.'\n",
    "                    self.scratchpad += f'Illegal Attraction Search. Please try again.'\n",
    "                    self.json_log[-1]['state'] = f'Illegal args. Other Error'\n",
    "\n",
    "            elif action_type == 'RestaurantSearch': #action_arg = 'Cheap Budget, Indian, [Good Flavor, Good Value]'\n",
    "                try:\n",
    "                    if validate_restaurant_parameters_format(action_arg):\n",
    "                        self.scratchpad = self.scratchpad.replace(to_string(self.current_data).strip(),'Masked due to limited length. Make sure the data has been written in Notebook.')\n",
    "                        self.current_data = self.tools['restaurants'].run(action_arg.split('[')[0].split(',')[0].strip(),action_arg.split('[')[0].split(',')[1].strip(),[a.strip() for a in action_arg.split('[')[1].strip()[:-1].split(',')])\n",
    "                        self.current_observation = str(to_string(self.current_data))\n",
    "                        self.scratchpad += 'AttractionSearch Succeeded' #self.current_observation\n",
    "                        self.notebook.append({'Description': 'Restaurant Choice', 'Content': self.current_data})\n",
    "                        self.__reset_record()\n",
    "                        self.json_log[-1]['state'] = f'Successful'\n",
    "                except ValueError as e:\n",
    "                    print(e)\n",
    "                    self.retry_record['restaurants'] += 1\n",
    "                    self.current_observation = str(e)\n",
    "                    self.scratchpad += str(e)\n",
    "                    self.json_log[-1]['state'] = f'Illegal args. Parameter Error'\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    self.retry_record['restaurants'] += 1\n",
    "                    self.current_observation = f'Illegal Restaurant Search. Please try again.'\n",
    "                    self.scratchpad += f'Illegal Restaurant Search. Please try again.'\n",
    "                    self.json_log[-1]['state'] = f'Illegal args. Other Error'\n",
    "\n",
    "            elif action_type == 'BusinessClusterSearch': #action_arg = 'Cheap Budget, Indian, [Good Flavor, Good Value]'\n",
    "                try:\n",
    "                    self.scratchpad = self.scratchpad.replace(to_string(self.current_data).strip(),'Masked due to limited length. Make sure the data has been written in Notebook.')\n",
    "                    self.current_data = self.tools['nearby'].run(self.notebook)\n",
    "                    self.current_observation = str(to_string(self.current_data))\n",
    "                    self.scratchpad += 'BusinessClusterSearch Succeeded' #self.current_observation\n",
    "                    self.notebook.append({'Description': 'Business Cluster Results', 'Content': self.current_data})\n",
    "                    self.__reset_record()\n",
    "                    self.json_log[-1]['state'] = f'Successful'\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    self.retry_record['nearby'] += 1\n",
    "                    self.current_observation = f'Illegal business cluster Search. Please try again.'\n",
    "                    self.scratchpad += f'Illegal business cluster Search. Please try again.'\n",
    "                    self.json_log[-1]['state'] = f'Illegal args. Other Error'\n",
    "\n",
    "            #elif action_type == 'NotebookWrite':\n",
    "            #    self.scratchpad = self.scratchpad.replace(to_string(self.current_data).strip(),'Masked due to limited length. Make sure the data has been written in Notebook.')\n",
    "            #    self.current_observation = str(self.tools['notebook'].write(self.current_data, action_arg))\n",
    "            #    self.scratchpad  +=  self.current_observation\n",
    "            #    self.json_log[-1]['state'] = f'Successful'\n",
    "\n",
    "            elif action_type == 'Planner':\n",
    "                self.current_observation = str(self.tools['planner'].run(self.notebook,action_arg))\n",
    "                self.scratchpad  +=  self.current_observation\n",
    "                self.answer = self.current_observation\n",
    "                self.json_log[-1]['state'] = f'Successful'\n",
    "            else:\n",
    "                self.retry_record['invalidAction'] += 1\n",
    "                self.current_observation = 'Invalid Action. Valid Actions are AccommodationSearch[Budget,Preference] / AttractionSearch[Budget, Preference] / RestaurantSearch[Budget, Cuisine, Preference]/ Planner[Query].'\n",
    "                self.scratchpad += self.current_observation\n",
    "                self.json_log[-1]['state'] = f'invalidAction'\n",
    "            \n",
    "            #print(f'Observation {self.step_n}: ' + self.current_observation+'\\n')\n",
    "            # rite(f'Observation {self.step_n}: ' + self.current_observation+'\\n')\n",
    "            #self.json_log[-1]['observation'] = self.current_observation\n",
    "            #self.step_n += 1\n",
    "\n",
    "            if action_type and action_type == 'Planner':\n",
    "                self.finished = True\n",
    "                self.answer = self.current_observation\n",
    "\n",
    "        if action == None or action == '' or action == '\\n':\n",
    "            print(f'Observation {self.step_n}: ' + \"No feedback from the environment due to the null action.\")\n",
    "            # write(f'Observation {self.step_n}: ' + \"Your action is filtered due to content. Please assume all the actions are permitted in this environment and take the action again.\")\n",
    "            self.json_log[-1]['observation'] = \"No feedback from the environment due to the null action.\"\n",
    "                #print(self.scratchpad)\n",
    "                #print(self.json_log)\n",
    "                #print(self.notebook) \n",
    "        else:\n",
    "            #print(f'Observation {self.step_n}: ' + self.current_observation+'\\n')\n",
    "            # rite(f'Observation {self.step_n}: ' + self.current_observation+'\\n')\n",
    "            self.json_log[-1]['observation'] = self.current_observation\n",
    "        \n",
    "        self.step_n += 1\n",
    "        if action_type and action_type == 'Planner' and self.retry_record['planner']==0:\n",
    "            \n",
    "            self.finished = True\n",
    "            self.answer = self.current_observation\n",
    "            self.step_n += 1\n",
    "            return\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def is_finished(self) -> bool:\n",
    "        return self.finished\n",
    "    \n",
    "    #def is_halted(self) -> bool:\n",
    "    #    return ((self.step_n > self.max_steps) or (\n",
    "    #                len(self.enc.encode(self._build_agent_prompt())) > self.max_token_length)) and not self.finished\n",
    "    \n",
    "    def __reset_agent(self) -> None:\n",
    "        self.step_n = 1\n",
    "        self.finished = False\n",
    "        self.answer = ''\n",
    "        self.scratchpad: str = ''\n",
    "        self.__reset_record()\n",
    "        self.json_log = []\n",
    "        self.current_observation = ''\n",
    "        self.current_data = None\n",
    "        self.last_actions = []\n",
    "        self.notebook = []\n",
    "\n",
    "    def prompt_agent(self) -> str:\n",
    "        #print(\"we prompt the agents\")\n",
    "        while True:\n",
    "            if self.react_name == 'gpt-4o-2024-11-20':\n",
    "                request = format_step('gpt-4o-2024-11-20',self.llm.invoke(self._build_agent_prompt()).content)\n",
    "            elif self.react_name == 'mistral-large-2411':\n",
    "                request = format_step('mistral-large-2411',self.llm.invoke(self._build_agent_prompt()).content.split('\\n')[0])\n",
    "            elif self.react_name == 'open-mixtral-8x7b':\n",
    "                request = format_step('open-mixtral-8x7b', self.llm.invoke(self._build_agent_prompt()).content.split('\\n')[0])\n",
    "            elif self.react_name == 'gemini-1.5-pro':\n",
    "                #print('we are here')\n",
    "                #print(self._build_agent_prompt())\n",
    "                request = format_step('gemini-1.5-pro',self.llm.invoke(self._build_agent_prompt()).content.split('\\n')[0])\n",
    "            else:\n",
    "                request = format_step('llama',self.llm(self._build_agent_prompt(), max_new_tokens = 256, return_full_text=False, do_sample=False)[0]['generated_text'].split('\\n')[0])\n",
    "            #print(\"here is the raw request: === \", request)\n",
    "            return request  \n",
    "        \n",
    "    def __reset_record(self) -> None:\n",
    "        self.retry_record = {key: 0 for key in self.retry_record}\n",
    "        self.retry_record['invalidAction'] = 0\n",
    "\n",
    "    def _build_agent_prompt(self) -> str:\n",
    "        if self.mode == \"zero_shot\":\n",
    "            return self.agent_prompt.format(\n",
    "                query=self.query,\n",
    "                scratchpad=self.scratchpad)\n",
    "        \n",
    "    def load_tools(self, tools: List[str], planner_model_name=None) -> Dict[str, Any]:\n",
    "        tools_map = {}\n",
    "        for tool_name in tools:\n",
    "            module = importlib.import_module(f\"tools.{tool_name}.apis\") #\n",
    "            \n",
    "            if tool_name == 'planner' and planner_model_name is not None:\n",
    "                tools_map[tool_name] = getattr(module, tool_name[0].upper()+tool_name[1:])(model_name=planner_model_name)\n",
    "            elif tool_name == 'nearby':\n",
    "                tools_map[tool_name] = getattr(module, tool_name[0].upper()+tool_name[1:])()\n",
    "            else:\n",
    "                tools_map[tool_name] = getattr(module, tool_name[0].upper()+tool_name[1:])(working_model = self.working_model)\n",
    "        #print(tools_map)\n",
    "        return tools_map\n",
    "        \n",
    "\n",
    "def format_step(model,step: str) -> str:\n",
    "    #return step.strip('\\n').strip().replace('\\n', '')\n",
    "    if model=='gemini-1.5-pro':\n",
    "        response = step.split(':')[-1].strip()\n",
    "        return response\n",
    "    else:\n",
    "        return step.strip('\\n').strip().replace('\\n', '')\n",
    "\n",
    "def parse_action(string):\n",
    "    if ('BusinessClusterSearch' not in string):\n",
    "        pattern = r'^(\\w+)\\[(.+)\\]$'\n",
    "        match = re.match(pattern, string)\n",
    "        action_type = match.group(1)\n",
    "        action_arg = match.group(2)\n",
    "    else:\n",
    "        action_type = 'BusinessClusterSearch'\n",
    "        action_arg = ''\n",
    "    #print(action_type,action_arg)\n",
    "    return action_type.strip(),action_arg.strip()\n",
    "\n",
    "#def parse action arg\n",
    "\n",
    "def to_string(data) -> str:\n",
    "    if data is not None:\n",
    "        if type(data) == DataFrame:\n",
    "            return data.to_string(index=False)\n",
    "        else:\n",
    "            return str(data)\n",
    "    else:\n",
    "        return str(None)\n",
    "    \n",
    "def validate_accommodation_parameters_format(action_arg):\n",
    "    pattern = r\"(.*\\s*.*)\\s*,\\s*\\[(.*)\\]\"\n",
    "    match = re.match(pattern, action_arg)\n",
    "    if not match:\n",
    "        raise ValueError(\"Parameter format not match. Please try again. Valid Format: Budget, preference list.\")\n",
    "    budget = match.group(1).lower()\n",
    "    preference_list = match.group(2)\n",
    "\n",
    "    budget_accepted = ['cheap budget', 'moderate budget','expensive budget']\n",
    "    budgetInRange = False\n",
    "    if budget in budget_accepted:\n",
    "        budgetInRange = True\n",
    "    if not budgetInRange:\n",
    "        raise ValueError(\"Wrong budget Input, valid ones include: cheap budget, moderate budget, and expensive budget. Please try again.\")\n",
    "\n",
    "    #preference\n",
    "    preference = preference_list.split(',')\n",
    "    preference_core = [p.lower().strip().split(' ')[-1].strip() for p in preference]\n",
    "    preferenceInRange = True\n",
    "    preferenceAccepted = ['location','service','safety','quality']\n",
    "    for p in preference_core:\n",
    "        if p not in preferenceAccepted:\n",
    "            preferenceInRange = False\n",
    "\n",
    "    if not preferenceInRange:\n",
    "        raise ValueError(\"Wrong preference Input. Please try again.\")\n",
    "    return True\n",
    "    \n",
    "def validate_attraction_parameters_format(action_arg):\n",
    "    pattern = r\"(.*\\s*.*)\\s*,\\s*\\[(.*)\\]\"\n",
    "    match = re.match(pattern, action_arg)\n",
    "    if not match:\n",
    "        raise ValueError(\"Parameter format not match. Please try again. Valid Format: Budget, Preference list.\")\n",
    "    budget = match.group(1).lower()\n",
    "    preference_list = match.group(2)\n",
    "\n",
    "    budget_accepted = ['cheap budget', 'moderate budget','expensive budget']\n",
    "    budgetInRange = False\n",
    "    if budget in budget_accepted:\n",
    "        budgetInRange = True\n",
    "    if not budgetInRange:\n",
    "        raise ValueError(\"Wrong budget Input, valid ones include: cheap budget, moderate budget, and expensive budget. Please try again.\")\n",
    "\n",
    "    preference = preference_list.strip().split(',')\n",
    "    if(len(preference) > 1 ):\n",
    "        raise ValueError(\"Attraction only allows one preference. Please try again\")\n",
    "    if '-' in preference[0]:\n",
    "        preference_core = preference[0].strip().split('-')[0].lower()\n",
    "    else:\n",
    "        preference_core = preference[0].strip().split(' ')[0].lower()\n",
    "    preferenceAccepted = [\"family\",\"history\",\"activity\",\"nature\",\"food\",\"shopping\"]\n",
    "    preferenceIsInRange = False\n",
    "    if(preference_core in preferenceAccepted):\n",
    "        preferenceIsInRange = True\n",
    "    if not preferenceIsInRange:\n",
    "        raise ValueError(\"Preference parameter invalid. Only family oriented / history oriented / activity oriented / nature oriented / food oriented / and shopping oriented are allowed. Please try again.\")\n",
    "    return True\n",
    "\n",
    "def validate_restaurant_parameters_format(action_arg):\n",
    "    pattern = r\"(.*\\s*.*),\\s*(.*),\\s*\\[(.*)\\]\"\n",
    "    match = re.match(pattern, action_arg)\n",
    "    if not match:\n",
    "        raise ValueError(\"Parameter format not match. Please try again. Valid Format: Budget, cuisine, preference list.\")\n",
    "    budget = match.group(1).lower()\n",
    "    cuisine = match.group(2).lower()\n",
    "    preference_list = match.group(3)\n",
    "\n",
    "    budget_accepted = ['cheap budget', 'moderate budget','expensive budget']\n",
    "    budgetInRange = False\n",
    "    #print(budget)\n",
    "    if budget in budget_accepted:\n",
    "        budgetInRange = True\n",
    "    if not budgetInRange:\n",
    "        raise ValueError(\"Wrong budget Input, valid ones include: cheap budget, moderate budget and expensive budget. Please try again.\")\n",
    "\n",
    "    cuisine_accepted = [\"us\",\"mexican\",\"irish\",\"french\",\"italian\",\"greek\",\"indian\",\"chinese\",\"japanese\",\"korean\",\"vietnamese\",\"thai\",\"asian fusion\",\"middle eastern\"]\n",
    "    #print(cuisine)\n",
    "    cuisineInRange = False\n",
    "    if cuisine in cuisine_accepted:\n",
    "        cuisineInRange = True\n",
    "    if not cuisineInRange:\n",
    "        raise ValueError(\"Cuisine not valid. Accepted cuisine is: US / Mexican / Irish / French / Italian / Greek / Indian / Chinese / Japanese / Korean / Vietnamese / Thai / Asian Fusion and Middle Eastern. Please try again.\")\n",
    "\n",
    "    preference_list = [p.lower().strip() for p in preference_list.split(',')]\n",
    "    preference_core = [p.strip().split(' ')[-1] for p in preference_list]\n",
    "    #print(preference_core)\n",
    "\n",
    "    preferenceInRange = True\n",
    "    preferenceAccepted = ['',\"flavor\",\"freshness\",\"service\",\"environment\",\"value\"]\n",
    "    for p in preference_core:\n",
    "        if p not in preferenceAccepted:\n",
    "            preferenceInRange = False\n",
    "\n",
    "    if not preferenceInRange:\n",
    "        raise ValueError(\"Wrong preference Input. Accepted inputs are: good flavor / good freshness / good healthy/ good service / good environment / good value. Please try again.\")    \n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tools_list = [\"attractions\",\"accommodations\",\"restaurants\",\"nearby\",\"planner\"]\n",
    "    modelList = ['gpt4o','mistral','llama318b','gemini']\n",
    "\n",
    "    model = modelList[3]\n",
    "    model_map = {'gpt4o': 'gpt-4o-2024-11-20', 'mistral':'mistral-large-2411','llama318b':'meta-llama/Llama-3.1-8B','gemini':'gemini-1.5-pro'}\n",
    "\n",
    "    agent = ReactAgent(working_model = 'gpt4o', tools=tools_list, react_llm_name = model_map[model], planner_llm_name = model_map[model])\n",
    "    toolUsePlans = []\n",
    "    toolUseScratchpads = []\n",
    "    toolUseLogs = []\n",
    "    with open (f'Prompts/humanQuerys.jsonl', 'r') as file:\n",
    "        humanquerys = [json.loads(line.strip()) for line in file]\n",
    "    for i in range (5):\n",
    "        query = humanquerys[i]['query']\n",
    "\n",
    "        planner_results, scratchpad, action_log  = agent.run(query)\n",
    "\n",
    "        toolUsePlans=[{\"index\": i+1, \"plan\": planner_results}]\n",
    "        toolUseScratchpads=[{\"index\": i+1, \"scratchpad\": scratchpad}]\n",
    "        toolUseLogs=[{\"index\": i+1, \"log\": action_log}]\n",
    "        \n",
    "        if(i%20 == 0):\n",
    "            print(f'done with plan: {i}')\n",
    "\n",
    "        with open (f'Output/{model}/plans/toolUsePlans.jsonl', 'a') as file:\n",
    "            for plan in toolUsePlans:\n",
    "                json.dump(plan, file)\n",
    "                file.write('\\n')\n",
    "        with open (f'Output/{model}/plans/toolUseScratchpads.jsonl', 'a') as file:\n",
    "            for scratchpad in toolUseScratchpads:\n",
    "                json.dump(scratchpad, file)\n",
    "                file.write('\\n')\n",
    "        with open (f'Output/{model}/plans/toolUseLogs.jsonl', 'a') as file:\n",
    "            for log in toolUseLogs:\n",
    "                json.dump(log, file)\n",
    "                file.write('\\n')\n",
    "    \"\"\"            \n",
    "    with open (f'Output/{model}/plans/toolUsePlans.jsonl', 'a') as file:\n",
    "        for plan in toolUsePlans:\n",
    "            json.dump(plan, file)\n",
    "            file.write('\\n')\n",
    "    with open (f'Output/{model}/plans/toolUseScratchpads.jsonl', 'a') as file:\n",
    "        for scratchpad in toolUseScratchpads:\n",
    "            json.dump(scratchpad, file)\n",
    "            file.write('\\n')\n",
    "    with open (f'Output/{model}/plans/toolUseLogs.jsonl', 'a') as file:\n",
    "        for log in toolUseLogs:\n",
    "            json.dump(log, file)\n",
    "            file.write('\\n')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
