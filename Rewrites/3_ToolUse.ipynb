{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought 1: To plan a 3-day trip with an expensive budget, focusing on nature-oriented attractions, US cuisine restaurants with good value, and a hotel with good service, I need to gather information on attractions, dining options, and accommodation.\n",
      "Action 1: AttractionSearch[Expensive budget,[Nature Oriented]]\n",
      "Thought 2: Now, I need to find suitable dining options that serve US cuisine and offer good value.\n",
      "Action 2: RestaurantSearch[Expensive budget, US, [Good Value]]\n",
      "Thought 3: Next, I should look for accommodation that offers good service.\n"
     ]
    },
    {
     "ename": "HTTPStatusError",
     "evalue": "Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 460\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m (\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    458\u001b[0m     query \u001b[38;5;241m=\u001b[39m humanquerys[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 460\u001b[0m     planner_results, scratchpad, action_log  \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    462\u001b[0m     toolUsePlans\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplan\u001b[39m\u001b[38;5;124m\"\u001b[39m: planner_results})\n\u001b[0;32m    463\u001b[0m     toolUseScratchpads\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscratchpad\u001b[39m\u001b[38;5;124m\"\u001b[39m: scratchpad})\n",
      "Cell \u001b[1;32mIn[1], line 75\u001b[0m, in \u001b[0;36mReactAgent.run\u001b[1;34m(self, query, reset)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reset_agent()\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_finished():\n\u001b[1;32m---> 75\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manswer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscratchpad, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjson_log\n",
      "Cell \u001b[1;32mIn[1], line 95\u001b[0m, in \u001b[0;36mReactAgent.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscratchpad \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAction \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_n\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m#get the action prompted\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m##\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m action \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m action \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m:\n",
      "Cell \u001b[1;32mIn[1], line 290\u001b[0m, in \u001b[0;36mReactAgent.prompt_agent\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    288\u001b[0m     request \u001b[38;5;241m=\u001b[39m format_step(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39minvoke([HumanMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_agent_prompt())])\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 290\u001b[0m     request \u001b[38;5;241m=\u001b[39m format_step(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_agent_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    291\u001b[0m \u001b[38;5;66;03m#print(\"here is the raw request: === \", request)\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m request\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torchgpu\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:286\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    282\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    283\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    285\u001b[0m         ChatGeneration,\n\u001b[1;32m--> 286\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    296\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torchgpu\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:786\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    780\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    784\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    785\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torchgpu\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[0;32m    642\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 643\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    644\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    645\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[0;32m    646\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[0;32m    647\u001b[0m ]\n\u001b[0;32m    648\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torchgpu\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 633\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    638\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    639\u001b[0m         )\n\u001b[0;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torchgpu\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:851\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    849\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    850\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 851\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    855\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torchgpu\\Lib\\site-packages\\langchain_mistralai\\chat_models.py:545\u001b[0m, in \u001b[0;36mChatMistralAI._generate\u001b[1;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[0;32m    543\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[0;32m    544\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m--> 545\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torchgpu\\Lib\\site-packages\\langchain_mistralai\\chat_models.py:464\u001b[0m, in \u001b[0;36mChatMistralAI.completion_with_retry\u001b[1;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    461\u001b[0m         _raise_on_error(response)\n\u001b[0;32m    462\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m--> 464\u001b[0m rtn \u001b[38;5;241m=\u001b[39m \u001b[43m_completion_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rtn\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torchgpu\\Lib\\site-packages\\langchain_mistralai\\chat_models.py:461\u001b[0m, in \u001b[0;36mChatMistralAI.completion_with_retry.<locals>._completion_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    460\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mpost(url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m, json\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 461\u001b[0m     \u001b[43m_raise_on_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    462\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\torchgpu\\Lib\\site-packages\\langchain_mistralai\\chat_models.py:170\u001b[0m, in \u001b[0;36m_raise_on_error\u001b[1;34m(response)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mcodes\u001b[38;5;241m.\u001b[39mis_error(response\u001b[38;5;241m.\u001b[39mstatus_code):\n\u001b[0;32m    169\u001b[0m     error_message \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError(\n\u001b[0;32m    171\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError response \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    172\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile fetching \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    173\u001b[0m         request\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mrequest,\n\u001b[0;32m    174\u001b[0m         response\u001b[38;5;241m=\u001b[39mresponse,\n\u001b[0;32m    175\u001b[0m     )\n",
      "\u001b[1;31mHTTPStatusError\u001b[0m: Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import importlib\n",
    "import json\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain.schema import HumanMessage\n",
    "from Utils.Prompt import zeroshot_react_agent_prompt\n",
    "from typing import List, Dict, Any\n",
    "from pandas import DataFrame\n",
    "\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPEN_AI_API')\n",
    "MISTRAL_API_KEY = os.getenv('MISTRAL_API_KEY')\n",
    "actionMapping = {\"AccommodationSearch\":\"accommodations\", \"RestaurantSearch\":\"restaurants\", \"AttractionSearch\":\"attraction\",\"BusinessClusterSearch\":\"nearby\",\"Planner\":\"planner\"}\n",
    "\n",
    "class ReactAgent:\n",
    "    def __init__(self,\n",
    "                 working_model,\n",
    "                 react_llm_name,\n",
    "                 planner_llm_name,\n",
    "                 #add clustering agent model name here as well\n",
    "                 mode: str = 'zero_shot',\n",
    "                 tools: List[str] = None,\n",
    "                 max_retries: int = 3,\n",
    "                 ) -> None: \n",
    "        self.react_name = react_llm_name\n",
    "        self.working_model = working_model\n",
    "        self.answer = ''\n",
    "        self.json_log = []\n",
    "        self.mode = mode\n",
    "        self.planner_name = planner_llm_name\n",
    "        self.notebook = []\n",
    "        self.max_retries = max_retries\n",
    "        self.last_actions = []\n",
    "        \n",
    "        self.current_observation = ''\n",
    "        self.current_data = None\n",
    "\n",
    "        self.tools = self.load_tools(tools, planner_model_name=planner_llm_name)\n",
    "        #print(self.tools)\n",
    "        self.retry_record = {key: 0 for key in self.tools}\n",
    "        #print(self.retry_record)\n",
    "        self.retry_record['invalidAction'] = 0\n",
    "        #print(self.retry_record)\n",
    "\n",
    "        if self.mode == 'zero_shot':\n",
    "            self.agent_prompt = zeroshot_react_agent_prompt\n",
    "\n",
    "        if 'gpt-4o' in react_llm_name:\n",
    "            stop_list = ['\\n']\n",
    "            self.max_token_length = 15000\n",
    "            self.llm = ChatOpenAI(temperature=0,\n",
    "                     max_tokens=256,\n",
    "                     model_name=react_llm_name,\n",
    "                     openai_api_key=OPENAI_API_KEY,\n",
    "                     model_kwargs={\"stop\": stop_list})\n",
    "        if 'mistral' in react_llm_name:\n",
    "            self.llm = ChatMistralAI(\n",
    "                model=\"mistral-large-2411\",\n",
    "                max_tokens=256,\n",
    "                temperature=0,\n",
    "                mistral_api_key = MISTRAL_API_KEY,\n",
    "                model_kwargs={\"stop\": ['\\n']}\n",
    "            )\n",
    "\n",
    "\n",
    "    def run(self,query, reset=True) -> None:\n",
    "        self.query = query\n",
    "\n",
    "        if reset:\n",
    "            self.__reset_agent()\n",
    "\n",
    "        while not self.is_finished():\n",
    "            self.step()\n",
    "        \n",
    "        return self.answer, self.scratchpad, self.json_log\n",
    "\n",
    "    \n",
    "    def step(self) -> None:\n",
    "        #record the log\n",
    "        self.json_log.append({\"step\": self.step_n, \"thought\":\"\",\n",
    "                              \"action\": \"\", \"observation\": \"\", \"state\":\"\"})\n",
    "        #thought\n",
    "        self.scratchpad += f'\\nThought {self.step_n}:'\n",
    "        self.scratchpad += ' ' + self.prompt_agent() #\n",
    "        print(self.scratchpad.split('\\n')[-1])\n",
    "        self.json_log[-1]['thought'] = self.scratchpad.split('\\n')[-1].replace(f'\\nThought {self.step_n}:',\"\")\n",
    "        \n",
    "        \n",
    "        #act\n",
    "        self.scratchpad += f'\\nAction {self.step_n}:'\n",
    "       \n",
    "        #get the action prompted\n",
    "        action = self.prompt_agent()\n",
    "        ##\n",
    "\n",
    "        if action == None or action == '' or action == '\\n':\n",
    "            self.scratchpad += \" Your action is filtered due to content. Please make sure your action does not start with ['\\\\n', 'Thought', 'Action', 'Observation'] and assume all the actions are permitted in this environment and take the action again.\"\n",
    "        else:\n",
    "            self.scratchpad += ' ' + action\n",
    "\n",
    "        if len(self.last_actions) > 0 and self.last_actions[-1] != action:\n",
    "            self.last_actions.clear()\n",
    "\n",
    "        # refresh last_action list\n",
    "        self.last_actions.append(action)\n",
    "\n",
    "        self.json_log[-1]['action'] = self.scratchpad.split('\\n')[-1].replace(f'\\nAction {self.step_n}:',\"\")\n",
    "\n",
    "        if len(self.last_actions) == 3:\n",
    "            print(\"The same action has been repeated 3 times consecutively. So we stop here.\")\n",
    "            # self.log_file.write(\"The same action has been repeated 3 times consecutively. So we stop here.\")\n",
    "            self.json_log[-1]['state'] = 'same action 3 times repeated'\n",
    "            self.finished = True\n",
    "            return\n",
    "\n",
    "\n",
    "        print(self.scratchpad.split('\\n')[-1])\n",
    "        \n",
    "        \n",
    "        #observation\n",
    "        self.scratchpad += f'\\nObservation {self.step_n}: '\n",
    "        action_type, action_arg = parse_action(action)\n",
    "        #print(action_type, action_arg)\n",
    "        if action_type != \"Planner\":\n",
    "            if action_type in actionMapping:\n",
    "                pending_action = actionMapping[action_type]\n",
    "            elif action_type not in actionMapping:\n",
    "                pending_action = 'invalidAction'\n",
    "\n",
    "            if pending_action in self.retry_record:\n",
    "                if self.retry_record[pending_action] + 1 > self.max_retries:\n",
    "                    action_type = 'Planner'\n",
    "                    print(f\"{pending_action} early stop due to {self.max_retries} max retries.\")\n",
    "                    self.json_log[-1]['state'] = f\"{pending_action} early stop due to {self.max_retries} max retries.\"\n",
    "                    self.finished = True\n",
    "                    return # so if the max tries is reached, we stop the loop\n",
    "            elif pending_action not in self.retry_record:\n",
    "                if self.retry_record['invalidAction'] + 1 > self.max_retries:\n",
    "                    action_type = 'Planner'\n",
    "                    print(f\"invalidAction Early stop due to {self.max_retries} max retries.\")\n",
    "                    # self.log_file.write(f\"invalidAction early stop due to {self.max_retries} max retries.\")\n",
    "                    self.json_log[-1]['state'] = f\"invalidAction early stop due to {self.max_retries} max retries.\"\n",
    "                    self.finished = True\n",
    "                    return\n",
    "\n",
    "        if action_type == 'AccommodationSearch':\n",
    "            try:\n",
    "                if validate_accommodation_parameters_format(action_arg):\n",
    "                    self.scratchpad = self.scratchpad.replace(to_string(self.current_data).strip(),'Masked due to limited length. Make sure the data has been written in Notebook.')\n",
    "                    self.current_data = self.tools['accommodations'].run(action_arg.split(',')[0],[p.strip() for p in action_arg.split('[')[1].strip('[]').split(',')])\n",
    "                    self.current_observation = str(to_string(self.current_data))\n",
    "                    self.scratchpad += 'AccommodationSearch Succeeded' #self.current_observation\n",
    "                    self.notebook.append({'Description': 'Accommodation Choice', 'Content': self.current_data})\n",
    "                    self.__reset_record()\n",
    "                    self.json_log[-1]['state'] = 'Successful'\n",
    "                    \n",
    "            except ValueError as e:\n",
    "                print(e)\n",
    "                self.retry_record['accommodations'] += 1\n",
    "                self.current_observation = str(e)\n",
    "                self.scratchpad += str(e)\n",
    "                self.json_log[-1]['state'] = f'Illegal args. Parameter Error'\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                self.retry_record['accommodations'] += 1\n",
    "                self.current_observation = f'Illegal Accommodation Search. Please try again.'\n",
    "                self.scratchpad += f'Illegal Accommodation Search. Please try again.'\n",
    "                self.json_log[-1]['state'] = f'Illegal args. Other Error'\n",
    "\n",
    "        elif action_type == 'AttractionSearch':\n",
    "            try:\n",
    "                if validate_attraction_parameters_format(action_arg):\n",
    "                    self.scratchpad = self.scratchpad.replace(to_string(self.current_data).strip(),'Masked due to limited length. Make sure the data has been written in Notebook.')\n",
    "                    self.current_data = self.tools['attractions'].run(action_arg.split(',')[0],[action_arg.split(',')[1].strip()[1:][:-1]])\n",
    "                    self.current_observation = str(to_string(self.current_data))\n",
    "                    self.scratchpad += 'AttractionSearch Succeeded' #self.current_observation \n",
    "                    self.notebook.append({'Description': 'Attraction Choice', 'Content': self.current_data})\n",
    "                    self.__reset_record()\n",
    "                    self.json_log[-1]['state'] = f'Successful'\n",
    "            except ValueError as e:\n",
    "                print(e)\n",
    "                self.retry_record['attractions'] += 1\n",
    "                self.current_observation = str(e)\n",
    "                self.scratchpad += str(e)\n",
    "                self.json_log[-1]['state'] = f'Illegal args. Parameter Error'\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                self.retry_record['attractions'] += 1\n",
    "                self.current_observation = f'Illegal Attraction Search. Please try again.'\n",
    "                self.scratchpad += f'Illegal Attraction Search. Please try again.'\n",
    "                self.json_log[-1]['state'] = f'Illegal args. Other Error'\n",
    "\n",
    "        elif action_type == 'RestaurantSearch': #action_arg = 'Cheap Budget, Indian, [Good Flavor, Good Value]'\n",
    "            try:\n",
    "                if validate_restaurant_parameters_format(action_arg):\n",
    "                    self.scratchpad = self.scratchpad.replace(to_string(self.current_data).strip(),'Masked due to limited length. Make sure the data has been written in Notebook.')\n",
    "                    self.current_data = self.tools['restaurants'].run(action_arg.split('[')[0].split(',')[0].strip(),action_arg.split('[')[0].split(',')[1].strip(),[a.strip() for a in action_arg.split('[')[1].strip()[:-1].split(',')])\n",
    "                    self.current_observation = str(to_string(self.current_data))\n",
    "                    self.scratchpad += 'AttractionSearch Succeeded' #self.current_observation\n",
    "                    self.notebook.append({'Description': 'Restaurant Choice', 'Content': self.current_data})\n",
    "                    self.__reset_record()\n",
    "                    self.json_log[-1]['state'] = f'Successful'\n",
    "            except ValueError as e:\n",
    "                print(e)\n",
    "                self.retry_record['restaurants'] += 1\n",
    "                self.current_observation = str(e)\n",
    "                self.scratchpad += str(e)\n",
    "                self.json_log[-1]['state'] = f'Illegal args. Parameter Error'\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                self.retry_record['restaurants'] += 1\n",
    "                self.current_observation = f'Illegal Restaurant Search. Please try again.'\n",
    "                self.scratchpad += f'Illegal Restaurant Search. Please try again.'\n",
    "                self.json_log[-1]['state'] = f'Illegal args. Other Error'\n",
    "\n",
    "        elif action_type == 'BusinessClusterSearch': #action_arg = 'Cheap Budget, Indian, [Good Flavor, Good Value]'\n",
    "            try:\n",
    "                self.scratchpad = self.scratchpad.replace(to_string(self.current_data).strip(),'Masked due to limited length. Make sure the data has been written in Notebook.')\n",
    "                self.current_data = self.tools['nearby'].run(self.notebook)\n",
    "                self.current_observation = str(to_string(self.current_data))\n",
    "                self.scratchpad += 'BusinessClusterSearch Succeeded' #self.current_observation\n",
    "                self.notebook.append({'Description': 'Restaurant Choice', 'Content': self.current_data})\n",
    "                self.__reset_record()\n",
    "                self.json_log[-1]['state'] = f'Successful'\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                self.retry_record['restaurants'] += 1\n",
    "                self.current_observation = f'Illegal Restaurant Search. Please try again.'\n",
    "                self.scratchpad += f'Illegal Restaurant Search. Please try again.'\n",
    "                self.json_log[-1]['state'] = f'Illegal args. Other Error'\n",
    "\n",
    "        #elif action_type == 'NotebookWrite':\n",
    "        #    self.scratchpad = self.scratchpad.replace(to_string(self.current_data).strip(),'Masked due to limited length. Make sure the data has been written in Notebook.')\n",
    "        #    self.current_observation = str(self.tools['notebook'].write(self.current_data, action_arg))\n",
    "        #    self.scratchpad  +=  self.current_observation\n",
    "        #    self.json_log[-1]['state'] = f'Successful'\n",
    "\n",
    "        elif action_type == 'Planner':\n",
    "            #print(self.notebook)\n",
    "            self.current_observation = str(self.tools['planner'].run(str(self.notebook),action_arg))\n",
    "            self.scratchpad  +=  self.current_observation\n",
    "            self.answer = self.current_observation\n",
    "            self.json_log[-1]['state'] = f'Successful'\n",
    "        else:\n",
    "            self.retry_record['invalidAction'] += 1\n",
    "            self.current_observation = 'Invalid Action. Valid Actions are AccommodationSearch[Budget,Preference] / AttractionSearch[Budget, Preference] / RestaurantSearch[Budget, Cuisine, Preference]/ Planner[Query].'\n",
    "            self.scratchpad += self.current_observation\n",
    "            self.json_log[-1]['state'] = f'invalidAction'\n",
    "        \n",
    "        #print(f'Observation {self.step_n}: ' + self.current_observation+'\\n')\n",
    "        # rite(f'Observation {self.step_n}: ' + self.current_observation+'\\n')\n",
    "        self.json_log[-1]['observation'] = self.current_observation\n",
    "        self.step_n += 1\n",
    "\n",
    "        if action_type and action_type == 'Planner':\n",
    "            self.finished = True\n",
    "            self.answer = self.current_observation\n",
    "\n",
    "            #print(self.scratchpad)\n",
    "            #print(self.json_log)\n",
    "            #print(self.notebook) \n",
    "        return\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def is_finished(self) -> bool:\n",
    "        return self.finished\n",
    "    \n",
    "    #def is_halted(self) -> bool:\n",
    "    #    return ((self.step_n > self.max_steps) or (\n",
    "    #                len(self.enc.encode(self._build_agent_prompt())) > self.max_token_length)) and not self.finished\n",
    "    \n",
    "    def __reset_agent(self) -> None:\n",
    "        self.step_n = 1\n",
    "        self.finished = False\n",
    "        self.answer = ''\n",
    "        self.scratchpad: str = ''\n",
    "        self.json_log = []\n",
    "\n",
    "    def prompt_agent(self) -> str:\n",
    "        #print(\"we prompt the agents\")\n",
    "        while True:\n",
    "            if self.react_name == 'gpt-4o-2024-11-20':\n",
    "                request = format_step(self.llm.invoke([HumanMessage(content=self._build_agent_prompt())]).content)\n",
    "            else:\n",
    "                request = format_step(self.llm.invoke(self._build_agent_prompt()).content.split('\\n')[0])\n",
    "            #print(\"here is the raw request: === \", request)\n",
    "            return request  \n",
    "        \n",
    "    def __reset_record(self) -> None:\n",
    "        self.retry_record = {key: 0 for key in self.retry_record}\n",
    "        self.retry_record['invalidAction'] = 0\n",
    "\n",
    "    def _build_agent_prompt(self) -> str:\n",
    "        if self.mode == \"zero_shot\":\n",
    "            return self.agent_prompt.format(\n",
    "                query=self.query,\n",
    "                scratchpad=self.scratchpad)\n",
    "        \n",
    "    def load_tools(self, tools: List[str], planner_model_name=None) -> Dict[str, Any]:\n",
    "        tools_map = {}\n",
    "        for tool_name in tools:\n",
    "            module = importlib.import_module(f\"tools.{tool_name}.apis\") #\n",
    "            \n",
    "            if tool_name == 'planner' and planner_model_name is not None:\n",
    "                tools_map[tool_name] = getattr(module, tool_name[0].upper()+tool_name[1:])(model_name=planner_model_name)\n",
    "            elif tool_name == 'nearby':\n",
    "                tools_map[tool_name] = getattr(module, tool_name[0].upper()+tool_name[1:])()\n",
    "            else:\n",
    "                tools_map[tool_name] = getattr(module, tool_name[0].upper()+tool_name[1:])(working_model = self.working_model)\n",
    "        #print(tools_map)\n",
    "        return tools_map\n",
    "        \n",
    "\n",
    "def format_step(step: str) -> str:\n",
    "    return step.strip('\\n').strip().replace('\\n', '')\n",
    "\n",
    "def parse_action(string):\n",
    "    if ('BusinessClusterSearch' not in string):\n",
    "        pattern = r'^(\\w+)\\[(.+)\\]$'\n",
    "        match = re.match(pattern, string)\n",
    "        action_type = match.group(1)\n",
    "        action_arg = match.group(2)\n",
    "    else:\n",
    "        action_type = 'BusinessClusterSearch'\n",
    "        action_arg = ''\n",
    "    return action_type,action_arg\n",
    "\n",
    "#def parse action arg\n",
    "\n",
    "def to_string(data) -> str:\n",
    "    if data is not None:\n",
    "        if type(data) == DataFrame:\n",
    "            return data.to_string(index=False)\n",
    "        else:\n",
    "            return str(data)\n",
    "    else:\n",
    "        return str(None)\n",
    "    \n",
    "def validate_accommodation_parameters_format(action_arg):\n",
    "    pattern = r\"(.*\\s*.*)\\s*,\\s*\\[(.*)\\]\"\n",
    "    match = re.match(pattern, action_arg)\n",
    "    if not match:\n",
    "        raise ValueError(\"Parameter format not match. Please try again. Valid Format: Budget, preference list.\")\n",
    "    budget = match.group(1).lower()\n",
    "    preference_list = match.group(2)\n",
    "\n",
    "    budget_accepted = ['cheap budget', 'moderate budget','expensive budget']\n",
    "    budgetInRange = False\n",
    "    if budget in budget_accepted:\n",
    "        budgetInRange = True\n",
    "    if not budgetInRange:\n",
    "        raise ValueError(\"Wrong budget Input, valid ones include: cheap budget, moderate budget, and expensive budget. Please try again.\")\n",
    "\n",
    "    #preference\n",
    "    preference = preference_list.split(',')\n",
    "    preference_core = [p.lower().strip().split(' ')[-1].strip() for p in preference]\n",
    "    preferenceInRange = True\n",
    "    preferenceAccepted = ['location','service','safety','quality']\n",
    "    for p in preference_core:\n",
    "        if p not in preferenceAccepted:\n",
    "            preferenceInRange = False\n",
    "\n",
    "    if not preferenceInRange:\n",
    "        raise ValueError(\"Wrong preference Input. Please try again.\")\n",
    "    return True\n",
    "    \n",
    "def validate_attraction_parameters_format(action_arg):\n",
    "    pattern = r\"(.*\\s*.*)\\s*,\\s*\\[(.*)\\]\"\n",
    "    match = re.match(pattern, action_arg)\n",
    "    if not match:\n",
    "        raise ValueError(\"Parameter format not match. Please try again. Valid Format: Budget, Preference list.\")\n",
    "    budget = match.group(1).lower()\n",
    "    preference_list = match.group(2)\n",
    "\n",
    "    budget_accepted = ['cheap budget', 'moderate budget','expensive budget']\n",
    "    budgetInRange = False\n",
    "    if budget in budget_accepted:\n",
    "        budgetInRange = True\n",
    "    if not budgetInRange:\n",
    "        raise ValueError(\"Wrong budget Input, valid ones include: cheap budget, moderate budget, and expensive budget. Please try again.\")\n",
    "\n",
    "    preference = preference_list.strip().split(',')\n",
    "    if(len(preference) > 1 ):\n",
    "        raise ValueError(\"Attraction only allows one preference. Please try again\")\n",
    "    if '-' in preference[0]:\n",
    "        preference_core = preference[0].strip().split('-')[0].lower()\n",
    "    else:\n",
    "        preference_core = preference[0].strip().split(' ')[0].lower()\n",
    "    preferenceAccepted = [\"family\",\"history\",\"activity\",\"nature\",\"food\",\"shopping\"]\n",
    "    preferenceIsInRange = False\n",
    "    if(preference_core in preferenceAccepted):\n",
    "        preferenceIsInRange = True\n",
    "    if not preferenceIsInRange:\n",
    "        raise ValueError(\"Preference parameter invalid. Only family oriented / history oriented / activity oriented / nature oriented / food oriented / and shopping oriented are allowed. Please try again.\")\n",
    "    return True\n",
    "\n",
    "def validate_restaurant_parameters_format(action_arg):\n",
    "    pattern = r\"(.*\\s*.*),\\s*(.*),\\s*\\[(.*)\\]\"\n",
    "    match = re.match(pattern, action_arg)\n",
    "    if not match:\n",
    "        raise ValueError(\"Parameter format not match. Please try again. Valid Format: Budget, cuisine, preference list.\")\n",
    "    budget = match.group(1).lower()\n",
    "    cuisine = match.group(2).lower()\n",
    "    preference_list = match.group(3)\n",
    "\n",
    "    budget_accepted = ['cheap budget', 'moderate budget','expensive budget']\n",
    "    budgetInRange = False\n",
    "    #print(budget)\n",
    "    if budget in budget_accepted:\n",
    "        budgetInRange = True\n",
    "    if not budgetInRange:\n",
    "        raise ValueError(\"Wrong budget Input, valid ones include: cheap budget, moderate budget and expensive budget. Please try again.\")\n",
    "\n",
    "    cuisine_accepted = [\"us\",\"mexican\",\"irish\",\"french\",\"italian\",\"greek\",\"indian\",\"chinese\",\"japanese\",\"korean\",\"vietnamese\",\"thai\",\"asian fusion\",\"middle eastern\"]\n",
    "    #print(cuisine)\n",
    "    cuisineInRange = False\n",
    "    if cuisine in cuisine_accepted:\n",
    "        cuisineInRange = True\n",
    "    if not cuisineInRange:\n",
    "        raise ValueError(\"Cuisine not valid. Accepted cuisine is: US / Mexican / Irish / French / Italian / Greek / Indian / Chinese / Japanese / Korean / Vietnamese / Thai / Asian Fusion and Middle Eastern. Please try again.\")\n",
    "\n",
    "    preference_list = [p.lower().strip() for p in preference_list.split(',')]\n",
    "    preference_core = [p.strip().split(' ')[-1] for p in preference_list]\n",
    "    #print(preference_core)\n",
    "\n",
    "    preferenceInRange = True\n",
    "    preferenceAccepted = ['',\"flavor\",\"freshness\",\"service\",\"environment\",\"value\"]\n",
    "    for p in preference_core:\n",
    "        if p not in preferenceAccepted:\n",
    "            preferenceInRange = False\n",
    "\n",
    "    if not preferenceInRange:\n",
    "        raise ValueError(\"Wrong preference Input. Accepted inputs are: good flavor / good freshness / good healthy/ good service / good environment / good value. Please try again.\")    \n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tools_list = [\"attractions\",\"accommodations\",\"restaurants\",\"nearby\",\"planner\"]\n",
    "    modelList = ['gpt4o','mistral']\n",
    "\n",
    "    model = modelList[0]\n",
    "    model_map = {'gpt4o': 'gpt-4o-2024-11-20', 'mistral':'mistral-large-2411'}\n",
    "\n",
    "    agent = ReactAgent(working_model = 'gpt4o', tools=tools_list, react_llm_name = model_map[model], planner_llm_name = model_map[model])\n",
    "    toolUsePlans = []\n",
    "    toolUseScratchpads = []\n",
    "    toolUseLogs = []\n",
    "    with open (f'Prompts/humanQuerys.jsonl', 'r') as file:\n",
    "        humanquerys = [json.loads(line.strip()) for line in file]\n",
    "    for i in range (500):\n",
    "        query = humanquerys[i]['query']\n",
    "\n",
    "        planner_results, scratchpad, action_log  = agent.run(query)\n",
    "\n",
    "        toolUsePlans.append({\"index\": i+1, \"plan\": planner_results})\n",
    "        toolUseScratchpads.append({\"index\": i+1, \"scratchpad\": scratchpad})\n",
    "        toolUseLogs.append({\"index\": i+1, \"log\": action_log})\n",
    "        \n",
    "        with open (f'Output/{model}/plans/toolUsePlans.jsonl', 'w') as file:\n",
    "            for plan in toolUsePlans:\n",
    "                json.dump(plan, file)\n",
    "                file.write('\\n')\n",
    "        with open (f'Output/{model}/plans/toolUseScratchpads.jsonl', 'w') as file:\n",
    "            for scratchpad in toolUseScratchpads:\n",
    "                json.dump(scratchpad, file)\n",
    "                file.write('\\n')\n",
    "        with open (f'Output/{model}/plans/toolUseLogs.jsonl', 'w') as file:\n",
    "            for log in toolUseLogs:\n",
    "                json.dump(log, file)\n",
    "                file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
