{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the model you choose is:  mistral  The planner LLM is:  mistral-large-2411\n",
      "The task you choose is:  filteredDataRouteOP\n",
      "we saved 1th plans\n",
      "we saved 21th plans\n",
      "we saved 41th plans\n",
      "we saved 61th plans\n",
      "we saved 81th plans\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import ollama\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain.schema import HumanMessage\n",
    "from Utils.Prompt import planner_no_route_agent, planner_route_OP_agent\n",
    "\n",
    "MISTRAL_API_KEY = os.getenv('MISTRAL_API_KEY')\n",
    "OPENAI_API_KEY = os.getenv('OPEN_AI_API')\n",
    "\n",
    "class SolePlanning:\n",
    "    def __init__(self,\n",
    "                 planner_llm) -> None:\n",
    "        \n",
    "        self.planner_llm = planner_llm\n",
    "\n",
    "        if self.planner_llm == 'gpt-4o-2024-11-20':\n",
    "            self.llm = ChatOpenAI(temperature=0,\n",
    "                        model_name='gpt-4o-2024-11-20',\n",
    "                        openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "        if self.planner_llm == 'mistral-large-2411':\n",
    "            self.llm = ChatMistralAI(\n",
    "                model=\"mistral-large-2411\",\n",
    "                temperature=0,\n",
    "                mistral_api_key = MISTRAL_API_KEY\n",
    "            )\n",
    "    def createPlan(self, prompt):\n",
    "        if self.planner_llm == 'gpt-4o-2024-11-20':\n",
    "            request = self.llm.invoke([HumanMessage(prompt)]).content\n",
    "            return request\n",
    "        if self.planner_llm == 'mistral-large-2411':\n",
    "            request = self.llm.invoke(prompt).content\n",
    "            return request\n",
    "\n",
    "        if self.planner_llm == 'llama3.1':\n",
    "            request = ollama.generate(model='llama3.1', prompt=prompt, options={'num_ctx': 70000})['response']\n",
    "            return request\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    modelList = ['gpt4o','mistral','llama318b']\n",
    "    model_map = {'gpt4o': 'gpt-4o-2024-11-20','mistral':'mistral-large-2411','llama318b':'llama3.1'}\n",
    "    taskList = ['allDataNoRoute', 'allDataRouteOP', 'filteredDataRouteOP','baseData']\n",
    "\n",
    "    #just choose the model and the task\n",
    "    #choose model\n",
    "    model = modelList[1]\n",
    "    planner_llm = model_map[model]\n",
    "    print(\"the model you choose is: \", model, \" The planner LLM is: \", planner_llm)\n",
    "    # choose task\n",
    "    task = taskList[2]\n",
    "    print(\"The task you choose is: \", task)\n",
    "    #number of plan\n",
    "    #numPlan = 300\n",
    "    #print(\"You will run: \" + str(numPlan) + \" plans\")\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    #load data (all/filtered)\n",
    "    if 'all' in task:\n",
    "        with open(f'Dataset/gpt4o/all_data.json', 'r') as file:\n",
    "            given_information = json.load(file)\n",
    "    elif 'filtered' in task:\n",
    "        with open(f'Dataset/gpt4o/filtered_data.jsonl', 'r') as file:\n",
    "            given_informations = [json.loads(line.strip()) for line in file]\n",
    "    else:\n",
    "        with open ('Dataset/Base/all_base_data.json', 'r') as file:\n",
    "            given_information = json.load(file)\n",
    "    \n",
    "    #iterate through all human querys\n",
    "    plans = []\n",
    "    with open ('Prompts/humanQuerys.jsonl', 'r') as file:\n",
    "            humanquerys = [json.loads(line.strip()) for line in file]\n",
    "    for i in range(100):\n",
    "        #load human query, allData only 100 plans, filteredData 500 plans.\n",
    "        query = humanquerys[i]['query']\n",
    "        #print(query)\n",
    "        \n",
    "    \n",
    "        #load prompt agent, and make the prompt\n",
    "        if task == 'allDataNoRoute':\n",
    "            prompt_agent = planner_no_route_agent\n",
    "            prompt = prompt_agent.format(given_information = given_information, query = query)\n",
    "            #print(prompt)\n",
    "        if task == 'allDataRouteOP':\n",
    "            prompt_agent = planner_route_OP_agent\n",
    "            prompt = prompt_agent.format(given_information = given_information, query = query)\n",
    "            #print(prompt)\n",
    "\n",
    "        if task == 'filteredDataRouteOP':\n",
    "            given_information = given_informations[i]['filtered_data']\n",
    "            prompt_agent = planner_route_OP_agent\n",
    "            prompt = prompt_agent.format(given_information = given_information, query = query)\n",
    "            #print(prompt)\n",
    "\n",
    "        if task == 'baseData':\n",
    "            prompt_agent = planner_no_route_agent\n",
    "            prompt = prompt_agent.format(given_information = given_information, query = query)\n",
    "        \n",
    "\n",
    "        #actual inference\n",
    "        solePlanning = SolePlanning(planner_llm=planner_llm)\n",
    "        plan = {\"index\": i+1, \"plan\": solePlanning.createPlan(prompt)}\n",
    "        plans = [plan]\n",
    "\n",
    "        with open (f'Output/{model}/plans/{task}.jsonl', 'a') as file:\n",
    "            for plan in plans:\n",
    "                file.write(json.dumps(plan) + '\\n')\n",
    "        if(i%20 == 0):\n",
    "            print(f\"we saved {i+1}th plans\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
